{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba2af70e-8373-4098-a8a3-b62509f5dbde",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"DISABLE_FLASH_ATTN\"] = \"1\"\n",
    "os.environ[\"DISABLE_FLASH_OPTIM\"] = \"1\"\n",
    "os.environ[\"FLASH_ATTENTION_DISABLE\"] = \"1\"\n",
    "os.environ[\"FLASH_ATTENTION_SKIP_CUDA_MEMORY_CHECK\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b4fbdd17-c90d-42af-9208-e95692999103",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-30 02:15:25.427793: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n2025-11-30 02:15:25.445643: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1764468925.468558    2141 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1764468925.476056    2141 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1764468925.495157    2141 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1764468925.495177    2141 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1764468925.495180    2141 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1764468925.495182    2141 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n2025-11-30 02:15:25.500856: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92f0e6c4ca114503a5a61fa436771971",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.12/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c507f66103164193823d658437d08ba2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/187 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration. Please open a PR/issue to update `preprocessor_config.json` to use `image_processor_type` instead of `feature_extractor_type`. This warning will be removed in v4.40.\n/local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.12/site-packages/torchvision/transforms/v2/_deprecated.py:42: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.Output is equivalent up to float precision.\n  warnings.warn(\n/databricks/python_shell/lib/dbruntime/huggingface_patches/datasets.py:55: UserWarning: The cache_dir for this dataset is /tmp/.hf.data.cache, which is not a persistent path.Therefore, if/when the cluster restarts, the downloaded dataset will be lost.The persistent storage options for this workspace/cluster config are: [DBFS, UC Volumes].Please update either `cache_dir` or the environment variable `HF_DATASETS_CACHE`to be under one of the following root directories: ['/dbfs/', '/Volumes/']\n  warnings.warn(warning_message)\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da54ff7443c942f1be675b66f9b56c4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python_shell/lib/dbruntime/huggingface_patches/datasets.py:24: UserWarning: During large dataset downloads, there could be multiple progress bar widgets that can cause performance issues for your notebook or browser. To avoid these issues, use `datasets.utils.logging.disable_progress_bar()` to turn off the progress bars.\n  warnings.warn(\nXet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b5f97a3a30849909bdbd0997fc6afc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)-00000-of-00005-3ac4e3b3fa8df68d.parquet:   0%|          | 0.00/457M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32fb82ba45c24010a4a3b89fce48ee86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)-00001-of-00005-72e716d68bb13413.parquet:   0%|          | 0.00/458M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c34c448f7e754f8180b88efbeee85778",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)-00002-of-00005-8dd97810f77d7a20.parquet:   0%|          | 0.00/458M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69922c75974d4ac9ad39baa1cc613361",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)-00003-of-00005-0e1eda89906f6ebb.parquet:   0%|          | 0.00/458M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da77de2cb52f45d7a91697c090c5ae7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)-00004-of-00005-cbbd007c0641d389.parquet:   0%|          | 0.00/457M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfb93027ae004bc2aa038952d6f5dd7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)-00000-of-00001-cef82967b9d8e57e.parquet:   0%|          | 0.00/255M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46d5e3e2910547b4941c3aa46aed1a2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/15962 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e69826a0e29426c8a82e0cbc2820567",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/1774 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aec4c9e3c2414444982cff2e35778170",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b54038dd52b64b8799ca82124a3f6c67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/22.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of MobileViTForImageClassification were not initialized from the model checkpoint at apple/mobilevit-small and are newly initialized because the shapes did not match:\n- classifier.weight: found shape torch.Size([1000, 640]) in the checkpoint and torch.Size([10, 640]) in the model instantiated\n- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([10]) in the model instantiated\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n2025/11/30 02:16:05 INFO mlflow.tracking.fluent: Experiment with name '/Shared/apple-mobilevit-small' does not exist. Creating a new experiment.\n/local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.12/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: \ndataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False)\n  warnings.warn(\n/local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.12/site-packages/accelerate/accelerator.py:463: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n[rank0]:[W1130 02:16:11.721378654 reducer.cpp:1457] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9980' max='9980' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9980/9980 1:06:10, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision Class 0</th>\n",
       "      <th>Recall Class 0</th>\n",
       "      <th>F1 Class 0</th>\n",
       "      <th>Support Class 0</th>\n",
       "      <th>Precision Class 1</th>\n",
       "      <th>Recall Class 1</th>\n",
       "      <th>F1 Class 1</th>\n",
       "      <th>Support Class 1</th>\n",
       "      <th>Precision Class 2</th>\n",
       "      <th>Recall Class 2</th>\n",
       "      <th>F1 Class 2</th>\n",
       "      <th>Support Class 2</th>\n",
       "      <th>Precision Class 3</th>\n",
       "      <th>Recall Class 3</th>\n",
       "      <th>F1 Class 3</th>\n",
       "      <th>Support Class 3</th>\n",
       "      <th>Precision Class 4</th>\n",
       "      <th>Recall Class 4</th>\n",
       "      <th>F1 Class 4</th>\n",
       "      <th>Support Class 4</th>\n",
       "      <th>Precision Class 5</th>\n",
       "      <th>Recall Class 5</th>\n",
       "      <th>F1 Class 5</th>\n",
       "      <th>Support Class 5</th>\n",
       "      <th>Precision Class 6</th>\n",
       "      <th>Recall Class 6</th>\n",
       "      <th>F1 Class 6</th>\n",
       "      <th>Support Class 6</th>\n",
       "      <th>Precision Class 7</th>\n",
       "      <th>Recall Class 7</th>\n",
       "      <th>F1 Class 7</th>\n",
       "      <th>Support Class 7</th>\n",
       "      <th>Precision Class 8</th>\n",
       "      <th>Recall Class 8</th>\n",
       "      <th>F1 Class 8</th>\n",
       "      <th>Support Class 8</th>\n",
       "      <th>Precision Class 9</th>\n",
       "      <th>Recall Class 9</th>\n",
       "      <th>F1 Class 9</th>\n",
       "      <th>Support Class 9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.550800</td>\n",
       "      <td>1.044642</td>\n",
       "      <td>0.652198</td>\n",
       "      <td>0.626978</td>\n",
       "      <td>0.652198</td>\n",
       "      <td>0.625598</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>109</td>\n",
       "      <td>0.758929</td>\n",
       "      <td>0.459459</td>\n",
       "      <td>0.572391</td>\n",
       "      <td>185</td>\n",
       "      <td>0.797872</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.845865</td>\n",
       "      <td>250</td>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.848485</td>\n",
       "      <td>0.770642</td>\n",
       "      <td>198</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.035714</td>\n",
       "      <td>0.068966</td>\n",
       "      <td>28</td>\n",
       "      <td>0.528889</td>\n",
       "      <td>0.548387</td>\n",
       "      <td>0.538462</td>\n",
       "      <td>217</td>\n",
       "      <td>0.552381</td>\n",
       "      <td>0.648045</td>\n",
       "      <td>0.596401</td>\n",
       "      <td>179</td>\n",
       "      <td>0.448276</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.502415</td>\n",
       "      <td>273</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.917197</td>\n",
       "      <td>0.854599</td>\n",
       "      <td>157</td>\n",
       "      <td>0.803371</td>\n",
       "      <td>0.803371</td>\n",
       "      <td>0.803371</td>\n",
       "      <td>178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.980100</td>\n",
       "      <td>0.828985</td>\n",
       "      <td>0.730552</td>\n",
       "      <td>0.742697</td>\n",
       "      <td>0.730552</td>\n",
       "      <td>0.700829</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.009174</td>\n",
       "      <td>0.018182</td>\n",
       "      <td>109</td>\n",
       "      <td>0.724638</td>\n",
       "      <td>0.810811</td>\n",
       "      <td>0.765306</td>\n",
       "      <td>185</td>\n",
       "      <td>0.784314</td>\n",
       "      <td>0.960000</td>\n",
       "      <td>0.863309</td>\n",
       "      <td>250</td>\n",
       "      <td>0.748971</td>\n",
       "      <td>0.919192</td>\n",
       "      <td>0.825397</td>\n",
       "      <td>198</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.535714</td>\n",
       "      <td>0.576923</td>\n",
       "      <td>28</td>\n",
       "      <td>0.679654</td>\n",
       "      <td>0.723502</td>\n",
       "      <td>0.700893</td>\n",
       "      <td>217</td>\n",
       "      <td>0.586364</td>\n",
       "      <td>0.720670</td>\n",
       "      <td>0.646617</td>\n",
       "      <td>179</td>\n",
       "      <td>0.630208</td>\n",
       "      <td>0.443223</td>\n",
       "      <td>0.520430</td>\n",
       "      <td>273</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.917197</td>\n",
       "      <td>0.886154</td>\n",
       "      <td>157</td>\n",
       "      <td>0.862637</td>\n",
       "      <td>0.882022</td>\n",
       "      <td>0.872222</td>\n",
       "      <td>178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.807700</td>\n",
       "      <td>0.684688</td>\n",
       "      <td>0.777339</td>\n",
       "      <td>0.782897</td>\n",
       "      <td>0.777339</td>\n",
       "      <td>0.761808</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.091743</td>\n",
       "      <td>0.161290</td>\n",
       "      <td>109</td>\n",
       "      <td>0.766169</td>\n",
       "      <td>0.832432</td>\n",
       "      <td>0.797927</td>\n",
       "      <td>185</td>\n",
       "      <td>0.854545</td>\n",
       "      <td>0.940000</td>\n",
       "      <td>0.895238</td>\n",
       "      <td>250</td>\n",
       "      <td>0.888350</td>\n",
       "      <td>0.924242</td>\n",
       "      <td>0.905941</td>\n",
       "      <td>198</td>\n",
       "      <td>0.692308</td>\n",
       "      <td>0.642857</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>28</td>\n",
       "      <td>0.870968</td>\n",
       "      <td>0.622120</td>\n",
       "      <td>0.725806</td>\n",
       "      <td>217</td>\n",
       "      <td>0.655340</td>\n",
       "      <td>0.754190</td>\n",
       "      <td>0.701299</td>\n",
       "      <td>179</td>\n",
       "      <td>0.584527</td>\n",
       "      <td>0.747253</td>\n",
       "      <td>0.655949</td>\n",
       "      <td>273</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.917197</td>\n",
       "      <td>0.902821</td>\n",
       "      <td>157</td>\n",
       "      <td>0.899441</td>\n",
       "      <td>0.904494</td>\n",
       "      <td>0.901961</td>\n",
       "      <td>178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.721700</td>\n",
       "      <td>0.672751</td>\n",
       "      <td>0.782976</td>\n",
       "      <td>0.785028</td>\n",
       "      <td>0.782976</td>\n",
       "      <td>0.774652</td>\n",
       "      <td>0.621622</td>\n",
       "      <td>0.211009</td>\n",
       "      <td>0.315068</td>\n",
       "      <td>109</td>\n",
       "      <td>0.832432</td>\n",
       "      <td>0.832432</td>\n",
       "      <td>0.832432</td>\n",
       "      <td>185</td>\n",
       "      <td>0.808219</td>\n",
       "      <td>0.944000</td>\n",
       "      <td>0.870849</td>\n",
       "      <td>250</td>\n",
       "      <td>0.898396</td>\n",
       "      <td>0.848485</td>\n",
       "      <td>0.872727</td>\n",
       "      <td>198</td>\n",
       "      <td>0.617647</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.677419</td>\n",
       "      <td>28</td>\n",
       "      <td>0.865979</td>\n",
       "      <td>0.774194</td>\n",
       "      <td>0.817518</td>\n",
       "      <td>217</td>\n",
       "      <td>0.603239</td>\n",
       "      <td>0.832402</td>\n",
       "      <td>0.699531</td>\n",
       "      <td>179</td>\n",
       "      <td>0.641791</td>\n",
       "      <td>0.630037</td>\n",
       "      <td>0.635860</td>\n",
       "      <td>273</td>\n",
       "      <td>0.899371</td>\n",
       "      <td>0.910828</td>\n",
       "      <td>0.905063</td>\n",
       "      <td>157</td>\n",
       "      <td>0.906433</td>\n",
       "      <td>0.870787</td>\n",
       "      <td>0.888252</td>\n",
       "      <td>178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.652300</td>\n",
       "      <td>0.612972</td>\n",
       "      <td>0.799887</td>\n",
       "      <td>0.793815</td>\n",
       "      <td>0.799887</td>\n",
       "      <td>0.790265</td>\n",
       "      <td>0.604167</td>\n",
       "      <td>0.266055</td>\n",
       "      <td>0.369427</td>\n",
       "      <td>109</td>\n",
       "      <td>0.810000</td>\n",
       "      <td>0.875676</td>\n",
       "      <td>0.841558</td>\n",
       "      <td>185</td>\n",
       "      <td>0.861818</td>\n",
       "      <td>0.948000</td>\n",
       "      <td>0.902857</td>\n",
       "      <td>250</td>\n",
       "      <td>0.860465</td>\n",
       "      <td>0.934343</td>\n",
       "      <td>0.895884</td>\n",
       "      <td>198</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.642857</td>\n",
       "      <td>0.692308</td>\n",
       "      <td>28</td>\n",
       "      <td>0.827751</td>\n",
       "      <td>0.797235</td>\n",
       "      <td>0.812207</td>\n",
       "      <td>217</td>\n",
       "      <td>0.642241</td>\n",
       "      <td>0.832402</td>\n",
       "      <td>0.725061</td>\n",
       "      <td>179</td>\n",
       "      <td>0.688034</td>\n",
       "      <td>0.589744</td>\n",
       "      <td>0.635108</td>\n",
       "      <td>273</td>\n",
       "      <td>0.915033</td>\n",
       "      <td>0.891720</td>\n",
       "      <td>0.903226</td>\n",
       "      <td>157</td>\n",
       "      <td>0.896739</td>\n",
       "      <td>0.926966</td>\n",
       "      <td>0.911602</td>\n",
       "      <td>178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.627800</td>\n",
       "      <td>0.616087</td>\n",
       "      <td>0.790868</td>\n",
       "      <td>0.783167</td>\n",
       "      <td>0.790868</td>\n",
       "      <td>0.780035</td>\n",
       "      <td>0.604167</td>\n",
       "      <td>0.266055</td>\n",
       "      <td>0.369427</td>\n",
       "      <td>109</td>\n",
       "      <td>0.812183</td>\n",
       "      <td>0.864865</td>\n",
       "      <td>0.837696</td>\n",
       "      <td>185</td>\n",
       "      <td>0.856631</td>\n",
       "      <td>0.956000</td>\n",
       "      <td>0.903592</td>\n",
       "      <td>250</td>\n",
       "      <td>0.835556</td>\n",
       "      <td>0.949495</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>198</td>\n",
       "      <td>0.633333</td>\n",
       "      <td>0.678571</td>\n",
       "      <td>0.655172</td>\n",
       "      <td>28</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.811060</td>\n",
       "      <td>0.798186</td>\n",
       "      <td>217</td>\n",
       "      <td>0.639269</td>\n",
       "      <td>0.782123</td>\n",
       "      <td>0.703518</td>\n",
       "      <td>179</td>\n",
       "      <td>0.699074</td>\n",
       "      <td>0.553114</td>\n",
       "      <td>0.617587</td>\n",
       "      <td>273</td>\n",
       "      <td>0.917197</td>\n",
       "      <td>0.917197</td>\n",
       "      <td>0.917197</td>\n",
       "      <td>157</td>\n",
       "      <td>0.877095</td>\n",
       "      <td>0.882022</td>\n",
       "      <td>0.879552</td>\n",
       "      <td>178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.592100</td>\n",
       "      <td>0.630725</td>\n",
       "      <td>0.799887</td>\n",
       "      <td>0.794202</td>\n",
       "      <td>0.799887</td>\n",
       "      <td>0.791923</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.321101</td>\n",
       "      <td>0.424242</td>\n",
       "      <td>109</td>\n",
       "      <td>0.811881</td>\n",
       "      <td>0.886486</td>\n",
       "      <td>0.847545</td>\n",
       "      <td>185</td>\n",
       "      <td>0.793443</td>\n",
       "      <td>0.968000</td>\n",
       "      <td>0.872072</td>\n",
       "      <td>250</td>\n",
       "      <td>0.845794</td>\n",
       "      <td>0.914141</td>\n",
       "      <td>0.878641</td>\n",
       "      <td>198</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.607143</td>\n",
       "      <td>0.708333</td>\n",
       "      <td>28</td>\n",
       "      <td>0.837438</td>\n",
       "      <td>0.783410</td>\n",
       "      <td>0.809524</td>\n",
       "      <td>217</td>\n",
       "      <td>0.691892</td>\n",
       "      <td>0.715084</td>\n",
       "      <td>0.703297</td>\n",
       "      <td>179</td>\n",
       "      <td>0.704724</td>\n",
       "      <td>0.655678</td>\n",
       "      <td>0.679317</td>\n",
       "      <td>273</td>\n",
       "      <td>0.904459</td>\n",
       "      <td>0.904459</td>\n",
       "      <td>0.904459</td>\n",
       "      <td>157</td>\n",
       "      <td>0.904494</td>\n",
       "      <td>0.904494</td>\n",
       "      <td>0.904494</td>\n",
       "      <td>178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.563300</td>\n",
       "      <td>0.553015</td>\n",
       "      <td>0.820744</td>\n",
       "      <td>0.818121</td>\n",
       "      <td>0.820744</td>\n",
       "      <td>0.818027</td>\n",
       "      <td>0.602410</td>\n",
       "      <td>0.458716</td>\n",
       "      <td>0.520833</td>\n",
       "      <td>109</td>\n",
       "      <td>0.822115</td>\n",
       "      <td>0.924324</td>\n",
       "      <td>0.870229</td>\n",
       "      <td>185</td>\n",
       "      <td>0.905882</td>\n",
       "      <td>0.924000</td>\n",
       "      <td>0.914851</td>\n",
       "      <td>250</td>\n",
       "      <td>0.901961</td>\n",
       "      <td>0.929293</td>\n",
       "      <td>0.915423</td>\n",
       "      <td>198</td>\n",
       "      <td>0.758621</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.771930</td>\n",
       "      <td>28</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.811060</td>\n",
       "      <td>0.798186</td>\n",
       "      <td>217</td>\n",
       "      <td>0.757962</td>\n",
       "      <td>0.664804</td>\n",
       "      <td>0.708333</td>\n",
       "      <td>179</td>\n",
       "      <td>0.682927</td>\n",
       "      <td>0.717949</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>273</td>\n",
       "      <td>0.959732</td>\n",
       "      <td>0.910828</td>\n",
       "      <td>0.934641</td>\n",
       "      <td>157</td>\n",
       "      <td>0.921348</td>\n",
       "      <td>0.921348</td>\n",
       "      <td>0.921348</td>\n",
       "      <td>178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.544000</td>\n",
       "      <td>0.558399</td>\n",
       "      <td>0.820744</td>\n",
       "      <td>0.817382</td>\n",
       "      <td>0.820744</td>\n",
       "      <td>0.811562</td>\n",
       "      <td>0.704545</td>\n",
       "      <td>0.284404</td>\n",
       "      <td>0.405229</td>\n",
       "      <td>109</td>\n",
       "      <td>0.860825</td>\n",
       "      <td>0.902703</td>\n",
       "      <td>0.881266</td>\n",
       "      <td>185</td>\n",
       "      <td>0.880597</td>\n",
       "      <td>0.944000</td>\n",
       "      <td>0.911197</td>\n",
       "      <td>250</td>\n",
       "      <td>0.882629</td>\n",
       "      <td>0.949495</td>\n",
       "      <td>0.914842</td>\n",
       "      <td>198</td>\n",
       "      <td>0.724138</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.736842</td>\n",
       "      <td>28</td>\n",
       "      <td>0.765182</td>\n",
       "      <td>0.870968</td>\n",
       "      <td>0.814655</td>\n",
       "      <td>217</td>\n",
       "      <td>0.672897</td>\n",
       "      <td>0.804469</td>\n",
       "      <td>0.732824</td>\n",
       "      <td>179</td>\n",
       "      <td>0.738197</td>\n",
       "      <td>0.630037</td>\n",
       "      <td>0.679842</td>\n",
       "      <td>273</td>\n",
       "      <td>0.929487</td>\n",
       "      <td>0.923567</td>\n",
       "      <td>0.926518</td>\n",
       "      <td>157</td>\n",
       "      <td>0.926136</td>\n",
       "      <td>0.915730</td>\n",
       "      <td>0.920904</td>\n",
       "      <td>178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.529500</td>\n",
       "      <td>0.560406</td>\n",
       "      <td>0.811161</td>\n",
       "      <td>0.805007</td>\n",
       "      <td>0.811161</td>\n",
       "      <td>0.801887</td>\n",
       "      <td>0.638298</td>\n",
       "      <td>0.275229</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>109</td>\n",
       "      <td>0.879781</td>\n",
       "      <td>0.870270</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>185</td>\n",
       "      <td>0.817568</td>\n",
       "      <td>0.968000</td>\n",
       "      <td>0.886447</td>\n",
       "      <td>250</td>\n",
       "      <td>0.840183</td>\n",
       "      <td>0.929293</td>\n",
       "      <td>0.882494</td>\n",
       "      <td>198</td>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.774194</td>\n",
       "      <td>28</td>\n",
       "      <td>0.812500</td>\n",
       "      <td>0.838710</td>\n",
       "      <td>0.825397</td>\n",
       "      <td>217</td>\n",
       "      <td>0.697436</td>\n",
       "      <td>0.759777</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>179</td>\n",
       "      <td>0.722449</td>\n",
       "      <td>0.648352</td>\n",
       "      <td>0.683398</td>\n",
       "      <td>273</td>\n",
       "      <td>0.940397</td>\n",
       "      <td>0.904459</td>\n",
       "      <td>0.922078</td>\n",
       "      <td>157</td>\n",
       "      <td>0.894444</td>\n",
       "      <td>0.904494</td>\n",
       "      <td>0.899441</td>\n",
       "      <td>178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.506200</td>\n",
       "      <td>0.531591</td>\n",
       "      <td>0.829200</td>\n",
       "      <td>0.828675</td>\n",
       "      <td>0.829200</td>\n",
       "      <td>0.820436</td>\n",
       "      <td>0.789474</td>\n",
       "      <td>0.275229</td>\n",
       "      <td>0.408163</td>\n",
       "      <td>109</td>\n",
       "      <td>0.881720</td>\n",
       "      <td>0.886486</td>\n",
       "      <td>0.884097</td>\n",
       "      <td>185</td>\n",
       "      <td>0.868132</td>\n",
       "      <td>0.948000</td>\n",
       "      <td>0.906310</td>\n",
       "      <td>250</td>\n",
       "      <td>0.845455</td>\n",
       "      <td>0.939394</td>\n",
       "      <td>0.889952</td>\n",
       "      <td>198</td>\n",
       "      <td>0.793103</td>\n",
       "      <td>0.821429</td>\n",
       "      <td>0.807018</td>\n",
       "      <td>28</td>\n",
       "      <td>0.824324</td>\n",
       "      <td>0.843318</td>\n",
       "      <td>0.833713</td>\n",
       "      <td>217</td>\n",
       "      <td>0.696078</td>\n",
       "      <td>0.793296</td>\n",
       "      <td>0.741514</td>\n",
       "      <td>179</td>\n",
       "      <td>0.732342</td>\n",
       "      <td>0.721612</td>\n",
       "      <td>0.726937</td>\n",
       "      <td>273</td>\n",
       "      <td>0.929936</td>\n",
       "      <td>0.929936</td>\n",
       "      <td>0.929936</td>\n",
       "      <td>157</td>\n",
       "      <td>0.926136</td>\n",
       "      <td>0.915730</td>\n",
       "      <td>0.920904</td>\n",
       "      <td>178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.486000</td>\n",
       "      <td>0.521931</td>\n",
       "      <td>0.829763</td>\n",
       "      <td>0.829656</td>\n",
       "      <td>0.829763</td>\n",
       "      <td>0.822985</td>\n",
       "      <td>0.772727</td>\n",
       "      <td>0.311927</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>109</td>\n",
       "      <td>0.873016</td>\n",
       "      <td>0.891892</td>\n",
       "      <td>0.882353</td>\n",
       "      <td>185</td>\n",
       "      <td>0.903475</td>\n",
       "      <td>0.936000</td>\n",
       "      <td>0.919450</td>\n",
       "      <td>250</td>\n",
       "      <td>0.887805</td>\n",
       "      <td>0.919192</td>\n",
       "      <td>0.903226</td>\n",
       "      <td>198</td>\n",
       "      <td>0.676471</td>\n",
       "      <td>0.821429</td>\n",
       "      <td>0.741935</td>\n",
       "      <td>28</td>\n",
       "      <td>0.801762</td>\n",
       "      <td>0.838710</td>\n",
       "      <td>0.819820</td>\n",
       "      <td>217</td>\n",
       "      <td>0.744444</td>\n",
       "      <td>0.748603</td>\n",
       "      <td>0.746518</td>\n",
       "      <td>179</td>\n",
       "      <td>0.711864</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.739437</td>\n",
       "      <td>273</td>\n",
       "      <td>0.958904</td>\n",
       "      <td>0.891720</td>\n",
       "      <td>0.924092</td>\n",
       "      <td>157</td>\n",
       "      <td>0.861538</td>\n",
       "      <td>0.943820</td>\n",
       "      <td>0.900804</td>\n",
       "      <td>178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.471200</td>\n",
       "      <td>0.522538</td>\n",
       "      <td>0.832582</td>\n",
       "      <td>0.829066</td>\n",
       "      <td>0.832582</td>\n",
       "      <td>0.827364</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.385321</td>\n",
       "      <td>0.497041</td>\n",
       "      <td>109</td>\n",
       "      <td>0.881720</td>\n",
       "      <td>0.886486</td>\n",
       "      <td>0.884097</td>\n",
       "      <td>185</td>\n",
       "      <td>0.872727</td>\n",
       "      <td>0.960000</td>\n",
       "      <td>0.914286</td>\n",
       "      <td>250</td>\n",
       "      <td>0.873239</td>\n",
       "      <td>0.939394</td>\n",
       "      <td>0.905109</td>\n",
       "      <td>198</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>28</td>\n",
       "      <td>0.865385</td>\n",
       "      <td>0.829493</td>\n",
       "      <td>0.847059</td>\n",
       "      <td>217</td>\n",
       "      <td>0.697917</td>\n",
       "      <td>0.748603</td>\n",
       "      <td>0.722372</td>\n",
       "      <td>179</td>\n",
       "      <td>0.722222</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.718232</td>\n",
       "      <td>273</td>\n",
       "      <td>0.941558</td>\n",
       "      <td>0.923567</td>\n",
       "      <td>0.932476</td>\n",
       "      <td>157</td>\n",
       "      <td>0.907609</td>\n",
       "      <td>0.938202</td>\n",
       "      <td>0.922652</td>\n",
       "      <td>178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.466000</td>\n",
       "      <td>0.526691</td>\n",
       "      <td>0.832018</td>\n",
       "      <td>0.829165</td>\n",
       "      <td>0.832018</td>\n",
       "      <td>0.825235</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.321101</td>\n",
       "      <td>0.443038</td>\n",
       "      <td>109</td>\n",
       "      <td>0.853535</td>\n",
       "      <td>0.913514</td>\n",
       "      <td>0.882507</td>\n",
       "      <td>185</td>\n",
       "      <td>0.904215</td>\n",
       "      <td>0.944000</td>\n",
       "      <td>0.923679</td>\n",
       "      <td>250</td>\n",
       "      <td>0.897059</td>\n",
       "      <td>0.924242</td>\n",
       "      <td>0.910448</td>\n",
       "      <td>198</td>\n",
       "      <td>0.657143</td>\n",
       "      <td>0.821429</td>\n",
       "      <td>0.730159</td>\n",
       "      <td>28</td>\n",
       "      <td>0.801724</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.828508</td>\n",
       "      <td>217</td>\n",
       "      <td>0.774390</td>\n",
       "      <td>0.709497</td>\n",
       "      <td>0.740525</td>\n",
       "      <td>179</td>\n",
       "      <td>0.708475</td>\n",
       "      <td>0.765568</td>\n",
       "      <td>0.735915</td>\n",
       "      <td>273</td>\n",
       "      <td>0.912500</td>\n",
       "      <td>0.929936</td>\n",
       "      <td>0.921136</td>\n",
       "      <td>157</td>\n",
       "      <td>0.920455</td>\n",
       "      <td>0.910112</td>\n",
       "      <td>0.915254</td>\n",
       "      <td>178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.458200</td>\n",
       "      <td>0.503151</td>\n",
       "      <td>0.830327</td>\n",
       "      <td>0.825780</td>\n",
       "      <td>0.830327</td>\n",
       "      <td>0.824379</td>\n",
       "      <td>0.678571</td>\n",
       "      <td>0.348624</td>\n",
       "      <td>0.460606</td>\n",
       "      <td>109</td>\n",
       "      <td>0.865979</td>\n",
       "      <td>0.908108</td>\n",
       "      <td>0.886544</td>\n",
       "      <td>185</td>\n",
       "      <td>0.883895</td>\n",
       "      <td>0.944000</td>\n",
       "      <td>0.912959</td>\n",
       "      <td>250</td>\n",
       "      <td>0.886792</td>\n",
       "      <td>0.949495</td>\n",
       "      <td>0.917073</td>\n",
       "      <td>198</td>\n",
       "      <td>0.741935</td>\n",
       "      <td>0.821429</td>\n",
       "      <td>0.779661</td>\n",
       "      <td>28</td>\n",
       "      <td>0.838710</td>\n",
       "      <td>0.838710</td>\n",
       "      <td>0.838710</td>\n",
       "      <td>217</td>\n",
       "      <td>0.712766</td>\n",
       "      <td>0.748603</td>\n",
       "      <td>0.730245</td>\n",
       "      <td>179</td>\n",
       "      <td>0.713768</td>\n",
       "      <td>0.721612</td>\n",
       "      <td>0.717668</td>\n",
       "      <td>273</td>\n",
       "      <td>0.940789</td>\n",
       "      <td>0.910828</td>\n",
       "      <td>0.925566</td>\n",
       "      <td>157</td>\n",
       "      <td>0.906077</td>\n",
       "      <td>0.921348</td>\n",
       "      <td>0.913649</td>\n",
       "      <td>178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.443700</td>\n",
       "      <td>0.506089</td>\n",
       "      <td>0.832018</td>\n",
       "      <td>0.827473</td>\n",
       "      <td>0.832018</td>\n",
       "      <td>0.826947</td>\n",
       "      <td>0.676923</td>\n",
       "      <td>0.403670</td>\n",
       "      <td>0.505747</td>\n",
       "      <td>109</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.908108</td>\n",
       "      <td>0.881890</td>\n",
       "      <td>185</td>\n",
       "      <td>0.887218</td>\n",
       "      <td>0.944000</td>\n",
       "      <td>0.914729</td>\n",
       "      <td>250</td>\n",
       "      <td>0.877358</td>\n",
       "      <td>0.939394</td>\n",
       "      <td>0.907317</td>\n",
       "      <td>198</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.724138</td>\n",
       "      <td>28</td>\n",
       "      <td>0.837963</td>\n",
       "      <td>0.834101</td>\n",
       "      <td>0.836028</td>\n",
       "      <td>217</td>\n",
       "      <td>0.721925</td>\n",
       "      <td>0.754190</td>\n",
       "      <td>0.737705</td>\n",
       "      <td>179</td>\n",
       "      <td>0.746154</td>\n",
       "      <td>0.710623</td>\n",
       "      <td>0.727955</td>\n",
       "      <td>273</td>\n",
       "      <td>0.959459</td>\n",
       "      <td>0.904459</td>\n",
       "      <td>0.931148</td>\n",
       "      <td>157</td>\n",
       "      <td>0.871134</td>\n",
       "      <td>0.949438</td>\n",
       "      <td>0.908602</td>\n",
       "      <td>178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.441500</td>\n",
       "      <td>0.508707</td>\n",
       "      <td>0.831454</td>\n",
       "      <td>0.829705</td>\n",
       "      <td>0.831454</td>\n",
       "      <td>0.825264</td>\n",
       "      <td>0.775510</td>\n",
       "      <td>0.348624</td>\n",
       "      <td>0.481013</td>\n",
       "      <td>109</td>\n",
       "      <td>0.870466</td>\n",
       "      <td>0.908108</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>185</td>\n",
       "      <td>0.886792</td>\n",
       "      <td>0.940000</td>\n",
       "      <td>0.912621</td>\n",
       "      <td>250</td>\n",
       "      <td>0.893204</td>\n",
       "      <td>0.929293</td>\n",
       "      <td>0.910891</td>\n",
       "      <td>198</td>\n",
       "      <td>0.741935</td>\n",
       "      <td>0.821429</td>\n",
       "      <td>0.779661</td>\n",
       "      <td>28</td>\n",
       "      <td>0.811404</td>\n",
       "      <td>0.852535</td>\n",
       "      <td>0.831461</td>\n",
       "      <td>217</td>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.737430</td>\n",
       "      <td>0.721311</td>\n",
       "      <td>179</td>\n",
       "      <td>0.715827</td>\n",
       "      <td>0.728938</td>\n",
       "      <td>0.722323</td>\n",
       "      <td>273</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.917197</td>\n",
       "      <td>0.920128</td>\n",
       "      <td>157</td>\n",
       "      <td>0.922652</td>\n",
       "      <td>0.938202</td>\n",
       "      <td>0.930362</td>\n",
       "      <td>178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.430700</td>\n",
       "      <td>0.497471</td>\n",
       "      <td>0.838782</td>\n",
       "      <td>0.835590</td>\n",
       "      <td>0.838782</td>\n",
       "      <td>0.834683</td>\n",
       "      <td>0.739130</td>\n",
       "      <td>0.467890</td>\n",
       "      <td>0.573034</td>\n",
       "      <td>109</td>\n",
       "      <td>0.870466</td>\n",
       "      <td>0.908108</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>185</td>\n",
       "      <td>0.878229</td>\n",
       "      <td>0.952000</td>\n",
       "      <td>0.913628</td>\n",
       "      <td>250</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.929293</td>\n",
       "      <td>0.908642</td>\n",
       "      <td>198</td>\n",
       "      <td>0.741935</td>\n",
       "      <td>0.821429</td>\n",
       "      <td>0.779661</td>\n",
       "      <td>28</td>\n",
       "      <td>0.820175</td>\n",
       "      <td>0.861751</td>\n",
       "      <td>0.840449</td>\n",
       "      <td>217</td>\n",
       "      <td>0.763636</td>\n",
       "      <td>0.703911</td>\n",
       "      <td>0.732558</td>\n",
       "      <td>179</td>\n",
       "      <td>0.744526</td>\n",
       "      <td>0.747253</td>\n",
       "      <td>0.745887</td>\n",
       "      <td>273</td>\n",
       "      <td>0.939597</td>\n",
       "      <td>0.891720</td>\n",
       "      <td>0.915033</td>\n",
       "      <td>157</td>\n",
       "      <td>0.893048</td>\n",
       "      <td>0.938202</td>\n",
       "      <td>0.915068</td>\n",
       "      <td>178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.423800</td>\n",
       "      <td>0.511927</td>\n",
       "      <td>0.837091</td>\n",
       "      <td>0.834719</td>\n",
       "      <td>0.837091</td>\n",
       "      <td>0.831591</td>\n",
       "      <td>0.754386</td>\n",
       "      <td>0.394495</td>\n",
       "      <td>0.518072</td>\n",
       "      <td>109</td>\n",
       "      <td>0.912088</td>\n",
       "      <td>0.897297</td>\n",
       "      <td>0.904632</td>\n",
       "      <td>185</td>\n",
       "      <td>0.866426</td>\n",
       "      <td>0.960000</td>\n",
       "      <td>0.910816</td>\n",
       "      <td>250</td>\n",
       "      <td>0.890476</td>\n",
       "      <td>0.944444</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>198</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.758621</td>\n",
       "      <td>28</td>\n",
       "      <td>0.814978</td>\n",
       "      <td>0.852535</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>217</td>\n",
       "      <td>0.702020</td>\n",
       "      <td>0.776536</td>\n",
       "      <td>0.737401</td>\n",
       "      <td>179</td>\n",
       "      <td>0.749020</td>\n",
       "      <td>0.699634</td>\n",
       "      <td>0.723485</td>\n",
       "      <td>273</td>\n",
       "      <td>0.941558</td>\n",
       "      <td>0.923567</td>\n",
       "      <td>0.932476</td>\n",
       "      <td>157</td>\n",
       "      <td>0.907609</td>\n",
       "      <td>0.938202</td>\n",
       "      <td>0.922652</td>\n",
       "      <td>178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.422400</td>\n",
       "      <td>0.513047</td>\n",
       "      <td>0.835964</td>\n",
       "      <td>0.831189</td>\n",
       "      <td>0.835964</td>\n",
       "      <td>0.829604</td>\n",
       "      <td>0.706897</td>\n",
       "      <td>0.376147</td>\n",
       "      <td>0.491018</td>\n",
       "      <td>109</td>\n",
       "      <td>0.854271</td>\n",
       "      <td>0.918919</td>\n",
       "      <td>0.885417</td>\n",
       "      <td>185</td>\n",
       "      <td>0.873188</td>\n",
       "      <td>0.964000</td>\n",
       "      <td>0.916350</td>\n",
       "      <td>250</td>\n",
       "      <td>0.889952</td>\n",
       "      <td>0.939394</td>\n",
       "      <td>0.914005</td>\n",
       "      <td>198</td>\n",
       "      <td>0.718750</td>\n",
       "      <td>0.821429</td>\n",
       "      <td>0.766667</td>\n",
       "      <td>28</td>\n",
       "      <td>0.814978</td>\n",
       "      <td>0.852535</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>217</td>\n",
       "      <td>0.769697</td>\n",
       "      <td>0.709497</td>\n",
       "      <td>0.738372</td>\n",
       "      <td>179</td>\n",
       "      <td>0.745387</td>\n",
       "      <td>0.739927</td>\n",
       "      <td>0.742647</td>\n",
       "      <td>273</td>\n",
       "      <td>0.946309</td>\n",
       "      <td>0.898089</td>\n",
       "      <td>0.921569</td>\n",
       "      <td>157</td>\n",
       "      <td>0.888298</td>\n",
       "      <td>0.938202</td>\n",
       "      <td>0.912568</td>\n",
       "      <td>178</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n/local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n  warnings.warn(  # warn only once\n/local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n  warnings.warn(  # warn only once\n/local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n  warnings.warn(  # warn only once\n/local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n  warnings.warn(  # warn only once\n/local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n  warnings.warn(  # warn only once\n/local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n  warnings.warn(  # warn only once\n/local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n  warnings.warn(  # warn only once\n/local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n  warnings.warn(  # warn only once\n/local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n  warnings.warn(  # warn only once\n/local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n  warnings.warn(  # warn only once\n/local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n  warnings.warn(  # warn only once\n/local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n  warnings.warn(  # warn only once\n/local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n  warnings.warn(  # warn only once\n/local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n  warnings.warn(  # warn only once\n/local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n  warnings.warn(  # warn only once\n/local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n  warnings.warn(  # warn only once\n/local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n  warnings.warn(  # warn only once\n/local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n  warnings.warn(  # warn only once\n/local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n  warnings.warn(  # warn only once\n/local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n  warnings.warn(  # warn only once\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='56' max='56' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [56/56 00:18]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.12/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "779e62149d0c4052845809379d75e843",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/255 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration. Please open a PR/issue to update `preprocessor_config.json` to use `image_processor_type` instead of `feature_extractor_type`. This warning will be removed in v4.40.\n/local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.12/site-packages/torchvision/transforms/v2/_deprecated.py:42: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.Output is equivalent up to float precision.\n  warnings.warn(\n/databricks/python_shell/lib/dbruntime/huggingface_patches/datasets.py:55: UserWarning: The cache_dir for this dataset is /tmp/.hf.data.cache, which is not a persistent path.Therefore, if/when the cluster restarts, the downloaded dataset will be lost.The persistent storage options for this workspace/cluster config are: [DBFS, UC Volumes].Please update either `cache_dir` or the environment variable `HF_DATASETS_CACHE`to be under one of the following root directories: ['/dbfs/', '/Volumes/']\n  warnings.warn(warning_message)\n/databricks/python_shell/lib/dbruntime/huggingface_patches/datasets.py:24: UserWarning: During large dataset downloads, there could be multiple progress bar widgets that can cause performance issues for your notebook or browser. To avoid these issues, use `datasets.utils.logging.disable_progress_bar()` to turn off the progress bars.\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3dcc1cb21324bd0bc3f7213f16f352c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08b7ed0c04074867882422f5059f21a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/113M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of SwinForImageClassification were not initialized from the model checkpoint at microsoft/swin-tiny-patch4-window7-224 and are newly initialized because the shapes did not match:\n- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([10]) in the model instantiated\n- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([10, 768]) in the model instantiated\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n2025/11/30 03:22:48 INFO mlflow.tracking.fluent: Experiment with name '/Shared/microsoft-swin-tiny-patch4-window7-224' does not exist. Creating a new experiment.\n/local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.12/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: \ndataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False)\n  warnings.warn(\n/local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.12/site-packages/accelerate/accelerator.py:463: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='11976' max='19960' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [11976/19960 52:28 < 34:59, 3.80 it/s, Epoch 12/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision Class 0</th>\n",
       "      <th>Recall Class 0</th>\n",
       "      <th>F1 Class 0</th>\n",
       "      <th>Support Class 0</th>\n",
       "      <th>Precision Class 1</th>\n",
       "      <th>Recall Class 1</th>\n",
       "      <th>F1 Class 1</th>\n",
       "      <th>Support Class 1</th>\n",
       "      <th>Precision Class 2</th>\n",
       "      <th>Recall Class 2</th>\n",
       "      <th>F1 Class 2</th>\n",
       "      <th>Support Class 2</th>\n",
       "      <th>Precision Class 3</th>\n",
       "      <th>Recall Class 3</th>\n",
       "      <th>F1 Class 3</th>\n",
       "      <th>Support Class 3</th>\n",
       "      <th>Precision Class 4</th>\n",
       "      <th>Recall Class 4</th>\n",
       "      <th>F1 Class 4</th>\n",
       "      <th>Support Class 4</th>\n",
       "      <th>Precision Class 5</th>\n",
       "      <th>Recall Class 5</th>\n",
       "      <th>F1 Class 5</th>\n",
       "      <th>Support Class 5</th>\n",
       "      <th>Precision Class 6</th>\n",
       "      <th>Recall Class 6</th>\n",
       "      <th>F1 Class 6</th>\n",
       "      <th>Support Class 6</th>\n",
       "      <th>Precision Class 7</th>\n",
       "      <th>Recall Class 7</th>\n",
       "      <th>F1 Class 7</th>\n",
       "      <th>Support Class 7</th>\n",
       "      <th>Precision Class 8</th>\n",
       "      <th>Recall Class 8</th>\n",
       "      <th>F1 Class 8</th>\n",
       "      <th>Support Class 8</th>\n",
       "      <th>Precision Class 9</th>\n",
       "      <th>Recall Class 9</th>\n",
       "      <th>F1 Class 9</th>\n",
       "      <th>Support Class 9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.954100</td>\n",
       "      <td>0.614923</td>\n",
       "      <td>0.790304</td>\n",
       "      <td>0.796477</td>\n",
       "      <td>0.790304</td>\n",
       "      <td>0.782296</td>\n",
       "      <td>0.628571</td>\n",
       "      <td>0.201835</td>\n",
       "      <td>0.305556</td>\n",
       "      <td>109</td>\n",
       "      <td>0.842391</td>\n",
       "      <td>0.837838</td>\n",
       "      <td>0.840108</td>\n",
       "      <td>185</td>\n",
       "      <td>0.849265</td>\n",
       "      <td>0.924000</td>\n",
       "      <td>0.885057</td>\n",
       "      <td>250</td>\n",
       "      <td>0.816425</td>\n",
       "      <td>0.853535</td>\n",
       "      <td>0.834568</td>\n",
       "      <td>198</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.754717</td>\n",
       "      <td>28</td>\n",
       "      <td>0.907975</td>\n",
       "      <td>0.682028</td>\n",
       "      <td>0.778947</td>\n",
       "      <td>217</td>\n",
       "      <td>0.631336</td>\n",
       "      <td>0.765363</td>\n",
       "      <td>0.691919</td>\n",
       "      <td>179</td>\n",
       "      <td>0.632653</td>\n",
       "      <td>0.794872</td>\n",
       "      <td>0.704545</td>\n",
       "      <td>273</td>\n",
       "      <td>0.915584</td>\n",
       "      <td>0.898089</td>\n",
       "      <td>0.906752</td>\n",
       "      <td>157</td>\n",
       "      <td>0.931034</td>\n",
       "      <td>0.910112</td>\n",
       "      <td>0.920455</td>\n",
       "      <td>178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.644900</td>\n",
       "      <td>0.552947</td>\n",
       "      <td>0.801015</td>\n",
       "      <td>0.811194</td>\n",
       "      <td>0.801015</td>\n",
       "      <td>0.794940</td>\n",
       "      <td>0.630435</td>\n",
       "      <td>0.266055</td>\n",
       "      <td>0.374194</td>\n",
       "      <td>109</td>\n",
       "      <td>0.891429</td>\n",
       "      <td>0.843243</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>185</td>\n",
       "      <td>0.938017</td>\n",
       "      <td>0.908000</td>\n",
       "      <td>0.922764</td>\n",
       "      <td>250</td>\n",
       "      <td>0.835616</td>\n",
       "      <td>0.924242</td>\n",
       "      <td>0.877698</td>\n",
       "      <td>198</td>\n",
       "      <td>0.851852</td>\n",
       "      <td>0.821429</td>\n",
       "      <td>0.836364</td>\n",
       "      <td>28</td>\n",
       "      <td>0.790393</td>\n",
       "      <td>0.834101</td>\n",
       "      <td>0.811659</td>\n",
       "      <td>217</td>\n",
       "      <td>0.813559</td>\n",
       "      <td>0.536313</td>\n",
       "      <td>0.646465</td>\n",
       "      <td>179</td>\n",
       "      <td>0.577320</td>\n",
       "      <td>0.820513</td>\n",
       "      <td>0.677761</td>\n",
       "      <td>273</td>\n",
       "      <td>0.887500</td>\n",
       "      <td>0.904459</td>\n",
       "      <td>0.895899</td>\n",
       "      <td>157</td>\n",
       "      <td>0.941176</td>\n",
       "      <td>0.898876</td>\n",
       "      <td>0.919540</td>\n",
       "      <td>178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.575700</td>\n",
       "      <td>0.516018</td>\n",
       "      <td>0.826381</td>\n",
       "      <td>0.829225</td>\n",
       "      <td>0.826381</td>\n",
       "      <td>0.820285</td>\n",
       "      <td>0.697674</td>\n",
       "      <td>0.275229</td>\n",
       "      <td>0.394737</td>\n",
       "      <td>109</td>\n",
       "      <td>0.865285</td>\n",
       "      <td>0.902703</td>\n",
       "      <td>0.883598</td>\n",
       "      <td>185</td>\n",
       "      <td>0.894118</td>\n",
       "      <td>0.912000</td>\n",
       "      <td>0.902970</td>\n",
       "      <td>250</td>\n",
       "      <td>0.931217</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.909561</td>\n",
       "      <td>198</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>0.678571</td>\n",
       "      <td>0.791667</td>\n",
       "      <td>28</td>\n",
       "      <td>0.842857</td>\n",
       "      <td>0.815668</td>\n",
       "      <td>0.829040</td>\n",
       "      <td>217</td>\n",
       "      <td>0.702564</td>\n",
       "      <td>0.765363</td>\n",
       "      <td>0.732620</td>\n",
       "      <td>179</td>\n",
       "      <td>0.665672</td>\n",
       "      <td>0.816850</td>\n",
       "      <td>0.733553</td>\n",
       "      <td>273</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.891720</td>\n",
       "      <td>0.912052</td>\n",
       "      <td>157</td>\n",
       "      <td>0.918478</td>\n",
       "      <td>0.949438</td>\n",
       "      <td>0.933702</td>\n",
       "      <td>178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.522600</td>\n",
       "      <td>0.498234</td>\n",
       "      <td>0.828636</td>\n",
       "      <td>0.828839</td>\n",
       "      <td>0.828636</td>\n",
       "      <td>0.826515</td>\n",
       "      <td>0.593407</td>\n",
       "      <td>0.495413</td>\n",
       "      <td>0.540000</td>\n",
       "      <td>109</td>\n",
       "      <td>0.905028</td>\n",
       "      <td>0.875676</td>\n",
       "      <td>0.890110</td>\n",
       "      <td>185</td>\n",
       "      <td>0.840278</td>\n",
       "      <td>0.968000</td>\n",
       "      <td>0.899628</td>\n",
       "      <td>250</td>\n",
       "      <td>0.861244</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.884521</td>\n",
       "      <td>198</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>28</td>\n",
       "      <td>0.860697</td>\n",
       "      <td>0.797235</td>\n",
       "      <td>0.827751</td>\n",
       "      <td>217</td>\n",
       "      <td>0.766234</td>\n",
       "      <td>0.659218</td>\n",
       "      <td>0.708709</td>\n",
       "      <td>179</td>\n",
       "      <td>0.710098</td>\n",
       "      <td>0.798535</td>\n",
       "      <td>0.751724</td>\n",
       "      <td>273</td>\n",
       "      <td>0.916129</td>\n",
       "      <td>0.904459</td>\n",
       "      <td>0.910256</td>\n",
       "      <td>157</td>\n",
       "      <td>0.958333</td>\n",
       "      <td>0.904494</td>\n",
       "      <td>0.930636</td>\n",
       "      <td>178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.496100</td>\n",
       "      <td>0.481262</td>\n",
       "      <td>0.824126</td>\n",
       "      <td>0.826401</td>\n",
       "      <td>0.824126</td>\n",
       "      <td>0.818573</td>\n",
       "      <td>0.680851</td>\n",
       "      <td>0.293578</td>\n",
       "      <td>0.410256</td>\n",
       "      <td>109</td>\n",
       "      <td>0.893855</td>\n",
       "      <td>0.864865</td>\n",
       "      <td>0.879121</td>\n",
       "      <td>185</td>\n",
       "      <td>0.895753</td>\n",
       "      <td>0.928000</td>\n",
       "      <td>0.911591</td>\n",
       "      <td>250</td>\n",
       "      <td>0.848214</td>\n",
       "      <td>0.959596</td>\n",
       "      <td>0.900474</td>\n",
       "      <td>198</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>28</td>\n",
       "      <td>0.870647</td>\n",
       "      <td>0.806452</td>\n",
       "      <td>0.837321</td>\n",
       "      <td>217</td>\n",
       "      <td>0.707071</td>\n",
       "      <td>0.782123</td>\n",
       "      <td>0.742706</td>\n",
       "      <td>179</td>\n",
       "      <td>0.662577</td>\n",
       "      <td>0.791209</td>\n",
       "      <td>0.721202</td>\n",
       "      <td>273</td>\n",
       "      <td>0.922581</td>\n",
       "      <td>0.910828</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>157</td>\n",
       "      <td>0.981366</td>\n",
       "      <td>0.887640</td>\n",
       "      <td>0.932153</td>\n",
       "      <td>178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.463400</td>\n",
       "      <td>0.502183</td>\n",
       "      <td>0.832582</td>\n",
       "      <td>0.833832</td>\n",
       "      <td>0.832582</td>\n",
       "      <td>0.826939</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.330275</td>\n",
       "      <td>0.458599</td>\n",
       "      <td>109</td>\n",
       "      <td>0.890625</td>\n",
       "      <td>0.924324</td>\n",
       "      <td>0.907162</td>\n",
       "      <td>185</td>\n",
       "      <td>0.871324</td>\n",
       "      <td>0.948000</td>\n",
       "      <td>0.908046</td>\n",
       "      <td>250</td>\n",
       "      <td>0.893401</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.891139</td>\n",
       "      <td>198</td>\n",
       "      <td>0.807692</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>28</td>\n",
       "      <td>0.850242</td>\n",
       "      <td>0.811060</td>\n",
       "      <td>0.830189</td>\n",
       "      <td>217</td>\n",
       "      <td>0.663677</td>\n",
       "      <td>0.826816</td>\n",
       "      <td>0.736318</td>\n",
       "      <td>179</td>\n",
       "      <td>0.730216</td>\n",
       "      <td>0.743590</td>\n",
       "      <td>0.736842</td>\n",
       "      <td>273</td>\n",
       "      <td>0.928105</td>\n",
       "      <td>0.904459</td>\n",
       "      <td>0.916129</td>\n",
       "      <td>157</td>\n",
       "      <td>0.938202</td>\n",
       "      <td>0.938202</td>\n",
       "      <td>0.938202</td>\n",
       "      <td>178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>0.470008</td>\n",
       "      <td>0.840474</td>\n",
       "      <td>0.837986</td>\n",
       "      <td>0.840474</td>\n",
       "      <td>0.837870</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.504587</td>\n",
       "      <td>0.558376</td>\n",
       "      <td>109</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.913514</td>\n",
       "      <td>0.889474</td>\n",
       "      <td>185</td>\n",
       "      <td>0.901141</td>\n",
       "      <td>0.948000</td>\n",
       "      <td>0.923977</td>\n",
       "      <td>250</td>\n",
       "      <td>0.880734</td>\n",
       "      <td>0.969697</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>198</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.830189</td>\n",
       "      <td>28</td>\n",
       "      <td>0.855072</td>\n",
       "      <td>0.815668</td>\n",
       "      <td>0.834906</td>\n",
       "      <td>217</td>\n",
       "      <td>0.715000</td>\n",
       "      <td>0.798883</td>\n",
       "      <td>0.754617</td>\n",
       "      <td>179</td>\n",
       "      <td>0.752988</td>\n",
       "      <td>0.692308</td>\n",
       "      <td>0.721374</td>\n",
       "      <td>273</td>\n",
       "      <td>0.910828</td>\n",
       "      <td>0.910828</td>\n",
       "      <td>0.910828</td>\n",
       "      <td>157</td>\n",
       "      <td>0.964706</td>\n",
       "      <td>0.921348</td>\n",
       "      <td>0.942529</td>\n",
       "      <td>178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.416400</td>\n",
       "      <td>0.490617</td>\n",
       "      <td>0.839910</td>\n",
       "      <td>0.846072</td>\n",
       "      <td>0.839910</td>\n",
       "      <td>0.839773</td>\n",
       "      <td>0.626374</td>\n",
       "      <td>0.522936</td>\n",
       "      <td>0.570000</td>\n",
       "      <td>109</td>\n",
       "      <td>0.885417</td>\n",
       "      <td>0.918919</td>\n",
       "      <td>0.901857</td>\n",
       "      <td>185</td>\n",
       "      <td>0.941176</td>\n",
       "      <td>0.896000</td>\n",
       "      <td>0.918033</td>\n",
       "      <td>250</td>\n",
       "      <td>0.824034</td>\n",
       "      <td>0.969697</td>\n",
       "      <td>0.890951</td>\n",
       "      <td>198</td>\n",
       "      <td>0.766667</td>\n",
       "      <td>0.821429</td>\n",
       "      <td>0.793103</td>\n",
       "      <td>28</td>\n",
       "      <td>0.918919</td>\n",
       "      <td>0.783410</td>\n",
       "      <td>0.845771</td>\n",
       "      <td>217</td>\n",
       "      <td>0.769697</td>\n",
       "      <td>0.709497</td>\n",
       "      <td>0.738372</td>\n",
       "      <td>179</td>\n",
       "      <td>0.693939</td>\n",
       "      <td>0.838828</td>\n",
       "      <td>0.759536</td>\n",
       "      <td>273</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.891720</td>\n",
       "      <td>0.912052</td>\n",
       "      <td>157</td>\n",
       "      <td>0.987500</td>\n",
       "      <td>0.887640</td>\n",
       "      <td>0.934911</td>\n",
       "      <td>178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.396000</td>\n",
       "      <td>0.447176</td>\n",
       "      <td>0.845547</td>\n",
       "      <td>0.841683</td>\n",
       "      <td>0.845547</td>\n",
       "      <td>0.842331</td>\n",
       "      <td>0.592593</td>\n",
       "      <td>0.440367</td>\n",
       "      <td>0.505263</td>\n",
       "      <td>109</td>\n",
       "      <td>0.929348</td>\n",
       "      <td>0.924324</td>\n",
       "      <td>0.926829</td>\n",
       "      <td>185</td>\n",
       "      <td>0.929134</td>\n",
       "      <td>0.944000</td>\n",
       "      <td>0.936508</td>\n",
       "      <td>250</td>\n",
       "      <td>0.882629</td>\n",
       "      <td>0.949495</td>\n",
       "      <td>0.914842</td>\n",
       "      <td>198</td>\n",
       "      <td>0.772727</td>\n",
       "      <td>0.607143</td>\n",
       "      <td>0.680000</td>\n",
       "      <td>28</td>\n",
       "      <td>0.868932</td>\n",
       "      <td>0.824885</td>\n",
       "      <td>0.846336</td>\n",
       "      <td>217</td>\n",
       "      <td>0.752874</td>\n",
       "      <td>0.731844</td>\n",
       "      <td>0.742210</td>\n",
       "      <td>179</td>\n",
       "      <td>0.731293</td>\n",
       "      <td>0.787546</td>\n",
       "      <td>0.758377</td>\n",
       "      <td>273</td>\n",
       "      <td>0.906832</td>\n",
       "      <td>0.929936</td>\n",
       "      <td>0.918239</td>\n",
       "      <td>157</td>\n",
       "      <td>0.913514</td>\n",
       "      <td>0.949438</td>\n",
       "      <td>0.931129</td>\n",
       "      <td>178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.375600</td>\n",
       "      <td>0.466779</td>\n",
       "      <td>0.842728</td>\n",
       "      <td>0.845536</td>\n",
       "      <td>0.842728</td>\n",
       "      <td>0.835896</td>\n",
       "      <td>0.765957</td>\n",
       "      <td>0.330275</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>109</td>\n",
       "      <td>0.886010</td>\n",
       "      <td>0.924324</td>\n",
       "      <td>0.904762</td>\n",
       "      <td>185</td>\n",
       "      <td>0.878676</td>\n",
       "      <td>0.956000</td>\n",
       "      <td>0.915709</td>\n",
       "      <td>250</td>\n",
       "      <td>0.866972</td>\n",
       "      <td>0.954545</td>\n",
       "      <td>0.908654</td>\n",
       "      <td>198</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.754717</td>\n",
       "      <td>28</td>\n",
       "      <td>0.864734</td>\n",
       "      <td>0.824885</td>\n",
       "      <td>0.844340</td>\n",
       "      <td>217</td>\n",
       "      <td>0.810811</td>\n",
       "      <td>0.670391</td>\n",
       "      <td>0.733945</td>\n",
       "      <td>179</td>\n",
       "      <td>0.684211</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.760976</td>\n",
       "      <td>273</td>\n",
       "      <td>0.944444</td>\n",
       "      <td>0.866242</td>\n",
       "      <td>0.903654</td>\n",
       "      <td>157</td>\n",
       "      <td>0.960674</td>\n",
       "      <td>0.960674</td>\n",
       "      <td>0.960674</td>\n",
       "      <td>178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.363200</td>\n",
       "      <td>0.478308</td>\n",
       "      <td>0.845547</td>\n",
       "      <td>0.848085</td>\n",
       "      <td>0.845547</td>\n",
       "      <td>0.841203</td>\n",
       "      <td>0.716981</td>\n",
       "      <td>0.348624</td>\n",
       "      <td>0.469136</td>\n",
       "      <td>109</td>\n",
       "      <td>0.932584</td>\n",
       "      <td>0.897297</td>\n",
       "      <td>0.914601</td>\n",
       "      <td>185</td>\n",
       "      <td>0.927711</td>\n",
       "      <td>0.924000</td>\n",
       "      <td>0.925852</td>\n",
       "      <td>250</td>\n",
       "      <td>0.857798</td>\n",
       "      <td>0.944444</td>\n",
       "      <td>0.899038</td>\n",
       "      <td>198</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>28</td>\n",
       "      <td>0.896552</td>\n",
       "      <td>0.838710</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>217</td>\n",
       "      <td>0.686636</td>\n",
       "      <td>0.832402</td>\n",
       "      <td>0.752525</td>\n",
       "      <td>179</td>\n",
       "      <td>0.741259</td>\n",
       "      <td>0.776557</td>\n",
       "      <td>0.758497</td>\n",
       "      <td>273</td>\n",
       "      <td>0.915584</td>\n",
       "      <td>0.898089</td>\n",
       "      <td>0.906752</td>\n",
       "      <td>157</td>\n",
       "      <td>0.954023</td>\n",
       "      <td>0.932584</td>\n",
       "      <td>0.943182</td>\n",
       "      <td>178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.341300</td>\n",
       "      <td>0.452499</td>\n",
       "      <td>0.857948</td>\n",
       "      <td>0.859122</td>\n",
       "      <td>0.857948</td>\n",
       "      <td>0.852401</td>\n",
       "      <td>0.795918</td>\n",
       "      <td>0.357798</td>\n",
       "      <td>0.493671</td>\n",
       "      <td>109</td>\n",
       "      <td>0.912568</td>\n",
       "      <td>0.902703</td>\n",
       "      <td>0.907609</td>\n",
       "      <td>185</td>\n",
       "      <td>0.904580</td>\n",
       "      <td>0.948000</td>\n",
       "      <td>0.925781</td>\n",
       "      <td>250</td>\n",
       "      <td>0.900943</td>\n",
       "      <td>0.964646</td>\n",
       "      <td>0.931707</td>\n",
       "      <td>198</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>28</td>\n",
       "      <td>0.889423</td>\n",
       "      <td>0.852535</td>\n",
       "      <td>0.870588</td>\n",
       "      <td>217</td>\n",
       "      <td>0.748634</td>\n",
       "      <td>0.765363</td>\n",
       "      <td>0.756906</td>\n",
       "      <td>179</td>\n",
       "      <td>0.729904</td>\n",
       "      <td>0.831502</td>\n",
       "      <td>0.777397</td>\n",
       "      <td>273</td>\n",
       "      <td>0.918239</td>\n",
       "      <td>0.929936</td>\n",
       "      <td>0.924051</td>\n",
       "      <td>157</td>\n",
       "      <td>0.964912</td>\n",
       "      <td>0.926966</td>\n",
       "      <td>0.945559</td>\n",
       "      <td>178</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n  warnings.warn(  # warn only once\n/local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n  warnings.warn(  # warn only once\n/local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n  warnings.warn(  # warn only once\n/local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n  warnings.warn(  # warn only once\n/local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n  warnings.warn(  # warn only once\n/local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n  warnings.warn(  # warn only once\n/local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n  warnings.warn(  # warn only once\n/local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n  warnings.warn(  # warn only once\n/local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n  warnings.warn(  # warn only once\n/local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n  warnings.warn(  # warn only once\n/local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n  warnings.warn(  # warn only once\n/local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n  warnings.warn(  # warn only once\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='111' max='111' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [111/111 00:28]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "configs = [\n",
    "    # \"/Workspace/Users/kevincajachuan@gmail.com/vpc3/configs/deit-small/config.json\",\n",
    "    # \"/Workspace/Users/kevincajachuan@gmail.com/vpc3/configs/convnext-tiny/config.json\",\n",
    "    \"/Workspace/Users/kevincajachuan@gmail.com/vpc3/configs/mobilevit-small/config.json\",\n",
    "    \"/Workspace/Users/kevincajachuan@gmail.com/vpc3/configs/swin-tiny/config.json\",\n",
    "]\n",
    "\n",
    "from app import main\n",
    "\n",
    "for config in configs:\n",
    "    main(config)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8490854215259201,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "2. Train",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}